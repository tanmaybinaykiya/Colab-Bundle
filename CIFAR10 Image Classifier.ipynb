{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL HW 1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/tanmaybinaykiya/Colab-Bundle/blob/master/CIFAR10%20Image%20Classifier.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "YRQcGAwgQONq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install Pytorch"
      ]
    },
    {
      "metadata": {
        "id": "1_pcD7rPPXdv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "print(\"platform, accelerator:\", platform, accelerator)\n",
        "!pip install -v -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o8RWuEYKQS1N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ]
    },
    {
      "metadata": {
        "id": "azqXp0vUkvh0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "import errno\n",
        "import numpy as np\n",
        "import sys\n",
        "if sys.version_info[0] == 2:\n",
        "    import cPickle as pickle\n",
        "else:\n",
        "    import pickle\n",
        "\n",
        "import torch.utils.data as data\n",
        "from torchvision.datasets.utils import download_url, check_integrity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "46Umcq22QZEC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Utilities"
      ]
    },
    {
      "metadata": {
        "id": "2_VEeJ6YQfJH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Time Util"
      ]
    },
    {
      "metadata": {
        "id": "98EDdOGQyLAW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import time\n",
        "\n",
        "def get_time():\n",
        "  ts = time.time()\n",
        "  return str(datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "get_time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g0Td1AyfQh4V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Google Drive Integration Utils"
      ]
    },
    {
      "metadata": {
        "id": "dCd8Cb_db_gd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "is_g_drive_setup = None\n",
        "def setup_googleDrive():\n",
        "  !pip install -U -q PyDrive\n",
        "  !git clone https://gist.github.com/dc7e60aa487430ea704a8cb3f2c5d6a6.git /tmp/colab_util_repo\n",
        "  !mv /tmp/colab_util_repo/colab_util.py colab_util.py \n",
        "  !rm -r /tmp/colab_util_repo\n",
        "  is_g_drive_setup = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gyPW4J1ZSqxY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "drive_handler = None\n",
        "def setup_drive_handler():\n",
        "  if not is_g_drive_setup:\n",
        "    setup_googleDrive()\n",
        "  drive_handler = GoogleDriveHandler()\n",
        "\n",
        "def upload_to_google_drive(filename):\n",
        "  if not drive_handler:\n",
        "    setup_drive_handler()\n",
        "  drive_handler.upload(filename, parent_path='test_folder')\n",
        "\n",
        "def download_from_google_drive(filename, local_file_name):\n",
        "  if not drive_handler:\n",
        "    setup_drive_handler()\n",
        "  drive_handler.download(filename, local_file_name)\n",
        "\n",
        "def test_g_drive():\n",
        "  !touch emptyFile2\n",
        "  !echo \"Hi\" > emptyFile2\n",
        "  upload_to_google_drive(\"emptyFile2\")\n",
        "  \n",
        "# test_g_drive()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ycdb-nH8Qm3d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CIFAR10 Dataset Utilities "
      ]
    },
    {
      "metadata": {
        "id": "UEm_hFESQA03",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CIFAR10(data.Dataset):\n",
        "    \"\"\"`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where directory\n",
        "            ``cifar-10-batches-py`` exists.\n",
        "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
        "            creates from test set.\n",
        "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\n",
        "            downloaded again.\n",
        "    \"\"\"\n",
        "    base_folder = 'cifar-10-batches-py'\n",
        "    url = \"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "    filename = \"cifar-10-python.tar.gz\"\n",
        "    tgz_md5 = 'c58f30108f718f92721af3b95e74349a'\n",
        "    # validation examples will come from here\n",
        "    train_list = [\n",
        "        ['data_batch_1', 'c99cafc152244af753f735de768cd75f'],\n",
        "        ['data_batch_2', 'd4bba439e000b95fd0a9bffe97cbabec'],\n",
        "        ['data_batch_3', '54ebc095f3ab1f0389bbae665268c751'],\n",
        "        ['data_batch_4', '634d18415352ddfa80567beed471001a'],\n",
        "        ['data_batch_5', '482c414d41f54cd18b22e5b47cb7c3cb'],\n",
        "    ]\n",
        "\n",
        "    test_list = [\n",
        "        ['test_batch', '40351d587109b95175f43aff81a1287e'],\n",
        "    ]\n",
        "\n",
        "    def __init__(self, root, split='train',\n",
        "                 transform=None, target_transform=None,\n",
        "                 download=False, val_samples=1000):\n",
        "        self.root = os.path.expanduser(root)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.split = split # train, val, or test\n",
        "\n",
        "        if download:\n",
        "            self.download()\n",
        "\n",
        "        if not self._check_integrity():\n",
        "            raise RuntimeError('Dataset not found or corrupted.' +\n",
        "                               ' You can use download=True to download it')\n",
        "\n",
        "        # now load the picked numpy arrays\n",
        "        if self.split in ['train', 'val']:\n",
        "            self.train_data = []\n",
        "            self.train_labels = []\n",
        "            for fentry in self.train_list:\n",
        "                f = fentry[0]\n",
        "                file = os.path.join(self.root, self.base_folder, f)\n",
        "                fo = open(file, 'rb')\n",
        "                if sys.version_info[0] == 2:\n",
        "                    entry = pickle.load(fo)\n",
        "                else:\n",
        "                    entry = pickle.load(fo, encoding='latin1')\n",
        "                self.train_data.append(entry['data'])\n",
        "                if 'labels' in entry:\n",
        "                    self.train_labels += entry['labels']\n",
        "                else:\n",
        "                    self.train_labels += entry['fine_labels']\n",
        "                fo.close()\n",
        "\n",
        "            self.train_data = np.concatenate(self.train_data)\n",
        "            self.train_data = self.train_data.reshape((50000, 3, 32, 32))\n",
        "            self.train_data = self.train_data.transpose((0, 2, 3, 1))  # convert to HWC\n",
        "            self.val_data = self.train_data[-val_samples:]\n",
        "            self.val_labels = self.train_labels[-val_samples:]\n",
        "            self.train_data = self.train_data[:-val_samples]\n",
        "            self.train_labels = self.train_labels[:-val_samples]\n",
        "        elif self.split == 'test':\n",
        "            f = self.test_list[0][0]\n",
        "            file = os.path.join(self.root, self.base_folder, f)\n",
        "            fo = open(file, 'rb')\n",
        "            if sys.version_info[0] == 2:\n",
        "                entry = pickle.load(fo)\n",
        "            else:\n",
        "                entry = pickle.load(fo, encoding='latin1')\n",
        "            self.test_data = entry['data']\n",
        "            if 'labels' in entry:\n",
        "                self.test_labels = entry['labels']\n",
        "            else:\n",
        "                self.test_labels = entry['fine_labels']\n",
        "            fo.close()\n",
        "            self.test_data = self.test_data.reshape((10000, 3, 32, 32))\n",
        "            self.test_data = self.test_data.transpose((0, 2, 3, 1))  # convert to HWC\n",
        "        else:\n",
        "            raise Exception('Unkown split {}'.format(self.split))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        if self.split == 'train':\n",
        "            img, target = self.train_data[index], self.train_labels[index]\n",
        "        elif self.split == 'val':\n",
        "            img, target = self.val_data[index], self.val_labels[index]\n",
        "        elif self.split == 'test':\n",
        "            img, target = self.test_data[index], self.test_labels[index]\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.split == 'train':\n",
        "            return len(self.train_data)\n",
        "        elif self.split == 'val':\n",
        "            return len(self.val_data)\n",
        "        elif self.split == 'test':\n",
        "            return len(self.test_data)\n",
        "\n",
        "    def _check_integrity(self):\n",
        "        root = self.root\n",
        "        for fentry in (self.train_list + self.test_list):\n",
        "            filename, md5 = fentry[0], fentry[1]\n",
        "            fpath = os.path.join(root, self.base_folder, filename)\n",
        "            if not check_integrity(fpath, md5):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def download(self):\n",
        "        import tarfile\n",
        "\n",
        "        if self._check_integrity():\n",
        "            print('Files already downloaded and verified')\n",
        "            return\n",
        "\n",
        "        root = self.root\n",
        "        download_url(self.url, root, self.filename, self.tgz_md5)\n",
        "\n",
        "        # extract file\n",
        "        cwd = os.getcwd()\n",
        "        tar = tarfile.open(os.path.join(root, self.filename), \"r:gz\")\n",
        "        os.chdir(root)\n",
        "        tar.extractall()\n",
        "        tar.close()\n",
        "        os.chdir(cwd)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6yiKdMZ_Qz64",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Core Implementation"
      ]
    },
    {
      "metadata": {
        "id": "HgjwQG1pPVo4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tmb8UKOMTEUg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train"
      ]
    },
    {
      "metadata": {
        "id": "X0SS02amLfXV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(epoch, model, train_loader, is_cuda, criterion, optimizer, log_interval, val_loader):\n",
        "    ret_strs = []\n",
        "    model.train()\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "#         print(\"Epoch: \", epoch, \"BatchIdx: \", batch_idx, batch[0].shape[0])\n",
        "        images, targets = Variable(batch[0]), Variable(batch[1])\n",
        "        if is_cuda:\n",
        "            images, targets = images.cuda(), targets.cuda()\n",
        "        loss = criterion(model(images), targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0: \n",
        "            val_loss, val_acc = evaluate(is_cuda=is_cuda, split='val', model=model, criterion=criterion, n_batches=4, loader=val_loader)\n",
        "            train_loss = loss.data[0]\n",
        "            examples_this_epoch = batch_idx * len(images)\n",
        "            epoch_progress = 100. * batch_idx / len(train_loader)\n",
        "            log_string = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\t Train Loss: {:.6f}\\tVal Loss: {:.6f}\\tVal Acc: {}'.format(\n",
        "                epoch, examples_this_epoch, len(train_loader.dataset), epoch_progress, train_loss, val_loss, val_acc)\n",
        "            print(log_string)\n",
        "            ret_strs.append(log_string)\n",
        "\n",
        "    return ret_strs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BX-END-hTGNU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ]
    },
    {
      "metadata": {
        "id": "r-dMpTJrN-3N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(is_cuda, split, model, loader, criterion, verbose=True, n_batches=None):\n",
        "    \"\"\"\n",
        "    Compute loss on val or test data.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "    n_examples = 0\n",
        "    for batch_i, batch in enumerate(loader):\n",
        "        data, target = batch\n",
        "        if is_cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        with torch.no_grad():\n",
        "            data, target = Variable(data), Variable(target)\n",
        "        output = model(data)\n",
        "        loss += criterion(output, target, size_average=False).data[0]\n",
        "        # predict the argmax of the log-probabilities\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "        n_examples += pred.size(0)\n",
        "        if n_batches and (batch_i >= n_batches):\n",
        "            break\n",
        "\n",
        "    loss /= n_examples\n",
        "    acc = 100. * correct / n_examples\n",
        "    if verbose:\n",
        "        print('{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "            split, loss, correct, n_examples, acc))\n",
        "    return loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AIq3P9D2TIVj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Plot Training curves"
      ]
    },
    {
      "metadata": {
        "id": "0N3RLvpguh5z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "# This is needed to save images \n",
        "# matplotlib.use('Agg')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "\n",
        "def parse_log_string(f):\n",
        "    # Parse the train and val losses one line at a time.\n",
        "\n",
        "    # regexes to find train and val losses on a line\n",
        "    float_regex = r'[-+]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?'\n",
        "    train_loss_re = re.compile('.*Train Loss: ({})'.format(float_regex))\n",
        "    val_loss_re = re.compile('.*Val Loss: ({})'.format(float_regex))\n",
        "    val_acc_re = re.compile('.*Val Acc: ({})'.format(float_regex))\n",
        "    \n",
        "    # extract one loss for each logged iteration\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "        \n",
        "    for line in f:\n",
        "        train_match = train_loss_re.match(line)\n",
        "        val_match = val_loss_re.match(line)\n",
        "        val_acc_match = val_acc_re.match(line)\n",
        "        if train_match:\n",
        "            train_losses.append(float(train_match.group(1)))\n",
        "        if val_match:\n",
        "            val_losses.append(float(val_match.group(1)))\n",
        "        if val_acc_match:\n",
        "            val_accs.append(float(val_acc_match.group(1)))\n",
        "    \n",
        "    return train_losses, val_losses, val_accs\n",
        "    \n",
        "def plot_curves(train_losses, val_losses, val_accs, clf_name):\n",
        "    fig = plt.figure()\n",
        "    plt.plot(train_losses, label='Train')\n",
        "    plt.plot(val_losses, label='Val')\n",
        "    plt.title(clf_name + 'Learning Curve')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    fig = plt.figure()\n",
        "    plt.plot(val_accs, label='Val')\n",
        "    plt.title(clf_name + ' Validation Accuracy During Training')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    \n",
        "def tr_curves(log_str, clf_name=\"My Model\"):\n",
        "    train_losses, val_losses, val_accs = parse_log_string(log_str)\n",
        "    plot_curves(train_losses, val_losses, val_accs, clf_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kP03Za2ERSHp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Core Executor\n",
        "\n",
        "- Loads the model\n",
        "- Loads the dataset\n",
        "- Defines the optimizer and loss function\n",
        "- Trains the model for provided number of epochs"
      ]
    },
    {
      "metadata": {
        "id": "kfCvJVr1N_A-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main(lr, momentum, epochs, model_name, hidden_dim, kernel_size, weight_decay=0.0, batch_size=512, seed=1, \n",
        "               test_batch_size=1000, log_interval=10, cifar_10_dir=\"data\", load_model_file=None):\n",
        "  \n",
        "  is_cuda = torch.cuda.is_available()\n",
        "  kwargs = {'num_workers': 1, 'pin_memory': True} if is_cuda else {}\n",
        "\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "  n_classes = 10\n",
        "  im_size = (3, 32, 32)\n",
        "\n",
        "  cifar10_mean_color = [0.49131522, 0.48209435, 0.44646862]\n",
        "  # std dev of color across training images\n",
        "  cifar10_std_color = [0.01897398, 0.03039277, 0.03872553]\n",
        "  \n",
        "  transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(cifar10_mean_color, cifar10_std_color),\n",
        "  ])\n",
        "  \n",
        "  # Model Loader\n",
        "  if load_model_file:\n",
        "    print(\"Loading model...\")\n",
        "    model = torch.load(load_model_file)\n",
        "    print(\"Loaded model...\")\n",
        "  else:\n",
        "    if model_name == 'softmax':\n",
        "      model = Softmax(im_size, n_classes)\n",
        "    elif model_name == 'twolayernn':\n",
        "        model = TwoLayerNN(im_size, hidden_dim, n_classes)\n",
        "    elif model_name == 'convnet':\n",
        "        model = CNN(im_size, hidden_dim, kernel_size, n_classes)\n",
        "    elif model_name == 'mymodel':\n",
        "        model = MyModel(im_size, hidden_dim, kernel_size, n_classes)\n",
        "    else:\n",
        "        raise Exception('Unknown model {}'.format(args.model))\n",
        "  \n",
        "  # Datasets\n",
        "  train_dataset = CIFAR10(cifar_10_dir, split='train', download=True, transform=transform)\n",
        "  val_dataset = CIFAR10(cifar_10_dir, split='val', download=True, transform=transform)\n",
        "  test_dataset = CIFAR10(cifar_10_dir, split='test', download=True, transform=transform)\n",
        "\n",
        "  # DataLoaders\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "  val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  if is_cuda:\n",
        "      model.cuda()\n",
        "\n",
        "  criterion = F.cross_entropy\n",
        "# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "  optimizer = optim.RMSprop(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "  scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5, 9, 12, 18], gamma=0.3)\n",
        "\n",
        "  ret_strs = []\n",
        "  for epoch in range(1, epochs + 1):\n",
        "      scheduler.step()\n",
        "      ret_strs = ret_strs + train(epoch, model, train_loader, is_cuda, criterion, optimizer, log_interval, val_loader)\n",
        "      file_name = model_name + \"_after5_\" + get_time() + str(epoch) + '.pt'\n",
        "      torch.save(model, file_name)\n",
        "\n",
        "  evaluate(is_cuda=is_cuda, split='test', criterion=criterion, verbose=True, model=model, loader=test_loader)\n",
        "  tr_curves(ret_strs)\n",
        "  torch.save(model, model_name+get_time()+'_final.pt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "envU9ODJQ58J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Definition"
      ]
    },
    {
      "metadata": {
        "id": "UOUv8Vt4Q82Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Softmax Classifier"
      ]
    },
    {
      "metadata": {
        "id": "gk_0rbjEO1s8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class Softmax(nn.Module):\n",
        "    def __init__(self, im_size, n_classes):\n",
        "        super(Softmax, self).__init__()\n",
        "        ch, h, w = im_size\n",
        "        C = n_classes\n",
        "        self.model = torch.nn.Sequential(\n",
        "            nn.Linear(ch * h * w, C, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "\n",
        "    def forward(self, images):\n",
        "        scores = None\n",
        "        N, C, H, W = images.shape\n",
        "        scores = self.model(images.reshape(N, C*H*W))\n",
        "        return scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W4glnLsdRA84",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Custom ConvNet Classifier\n",
        "\n",
        "**Model Description**\n",
        "\n",
        "- Input \n",
        "- [ Conv[512 x 3 x 3]  - ReLU - BatchNorm - MaxPool ] \n",
        "- [ Conv[1024 x 3 x 3]  - ReLU - BatchNorm - MaxPool ] \n",
        "- [ Conv[1536 x 3 x 3]  - ReLU - BatchNorm - MaxPool ] \n",
        "- [ Conv[2048 x 3 x 3]  - ReLU - BatchNorm - MaxPool ] \n",
        "- [ FC[8192 x 4096 ] - ReLU -BatchNorm ] \n",
        "- [ FC[4096 x 2048 ] - ReLU -BatchNorm ]  \n",
        "- [ FC[2048 x C ] - ReLU -BatchNorm ] \n",
        "- Softmax"
      ]
    },
    {
      "metadata": {
        "id": "x2jFFRTRqmqx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyModel(nn.Module):\n",
        "  def __init__(self, im_size, hidden_dim, kernel_size, n_classes):\n",
        "      \"\"\"\n",
        "      Extra credit model\n",
        "\n",
        "      Arguments:\n",
        "          im_size (tuple): A tuple of ints with (channels, height, width)\n",
        "          hidden_dim (int): Number of hidden activations to use\n",
        "          kernel_size (int): Width and height of (square) convolution filters\n",
        "          n_classes (int): Number of classes to score\n",
        "      \"\"\"\n",
        "      super(MyModel, self).__init__()\n",
        "      ch, h, w = im_size\n",
        "      C = n_classes\n",
        "\n",
        "      self.conv_block_1 = torch.nn.Sequential(\n",
        "          nn.Conv2d(in_channels=ch, out_channels=512, kernel_size=(kernel_size, kernel_size), padding=1),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm2d(512),\n",
        "#           nn.MaxPool2d(kernel_size=(2, 2))\n",
        "      )\n",
        "\n",
        "      self.conv_block_2 = torch.nn.Sequential(\n",
        "          nn.Conv2d(in_channels=512, out_channels=512 * 2, kernel_size=(kernel_size, kernel_size), padding=1),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm2d(512 * 2),\n",
        "          nn.MaxPool2d(kernel_size=(3, 3))\n",
        "      )\n",
        "\n",
        "      self.conv_block_3 = torch.nn.Sequential(\n",
        "          nn.Conv2d(in_channels=512 * 2, out_channels=512 * 3, kernel_size=(kernel_size, kernel_size), padding=1),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm2d(512 * 3),\n",
        "#           nn.MaxPool2d(kernel_size=(2, 2))\n",
        "      )\n",
        "\n",
        "      self.conv_block_4 = torch.nn.Sequential(\n",
        "          nn.Conv2d(in_channels=512 * 3, out_channels=512 * 4, kernel_size=(kernel_size, kernel_size), padding=1),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm2d(512 * 4),\n",
        "          nn.MaxPool2d(kernel_size=(3, 3))\n",
        "      )\n",
        "\n",
        "      self.conv_blocks = nn.Sequential(\n",
        "          self.conv_block_1,\n",
        "          self.conv_block_2,\n",
        "          self.conv_block_3,\n",
        "          self.conv_block_4\n",
        "      )\n",
        "\n",
        "      nn.init.xavier_normal_(self.conv_blocks[0][0].weight)\n",
        "      nn.init.xavier_normal_(self.conv_blocks[1][0].weight)\n",
        "      nn.init.xavier_normal_(self.conv_blocks[2][0].weight)\n",
        "      nn.init.xavier_normal_(self.conv_blocks[3][0].weight)\n",
        "\n",
        "      self.fcn_1 = torch.nn.Sequential(\n",
        "          nn.Linear(512 * 4 * 3 * 3, 8192),\n",
        "          nn.ReLU(),\n",
        "\n",
        "          nn.Linear(8192, 4096),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm1d(4096),\n",
        "          \n",
        "          nn.Linear(4096, 2048),\n",
        "          nn.ReLU(),\n",
        "          \n",
        "          nn.Linear(2048, C),\n",
        "#           nn.ReLU(),\n",
        "          nn.Softmax(1)\n",
        "      )\n",
        "      nn.init.xavier_normal_(self.fcn_1[0].weight)\n",
        "      nn.init.xavier_normal_(self.fcn_1[2].weight)\n",
        "      nn.init.xavier_normal_(self.fcn_1[5].weight)\n",
        "      nn.init.xavier_normal_(self.fcn_1[7].weight)\n",
        "\n",
        "  def forward(self, images):\n",
        "      scores = None\n",
        "      N = images.shape[0]\n",
        "      scores = self.fcn_1(self.conv_blocks(images).reshape(N, -1))\n",
        "      return scores\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4vJ_qcwXUg0V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Executor"
      ]
    },
    {
      "metadata": {
        "id": "8ccMicGyOHq5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch as torch\n",
        "main(lr = 1e-6, \n",
        "     momentum = 0.9, \n",
        "     weight_decay = 1e-4, \n",
        "     batch_size = 64, \n",
        "     epochs = 20, \n",
        "     model_name = \"mymodel\", \n",
        "     hidden_dim = 50, \n",
        "     kernel_size = 3,\n",
        "     seed = 1,            \n",
        "     test_batch_size = 1000, \n",
        "     log_interval = 30, \n",
        "     cifar_10_dir = \"data\"\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kGzAbFK88VNA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# upload_to_google_drive(\"mymodel_after5_2018-10-02 21:37:1917.pt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RaPMiZ7R3ww3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluation "
      ]
    },
    {
      "metadata": {
        "id": "vDG7qE062ksf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !/opt/bin/nvidia-smi\n",
        "# !ps -few\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XJnib3Mr3aKs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# NOTE: The scaffolding code for this part of the assignment\n",
        "# is adapted from https://github.com/pytorch/examples.\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import urllib\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mej8G4Ok3Z9G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ChallengeData(data.Dataset):\n",
        "    \"\"\"`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where directory\n",
        "            ``test_images.npy`` exists.\n",
        "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\n",
        "            downloaded again.\n",
        "    \"\"\"\n",
        "    url = \"https://s3.amazonaws.com/cs7643-fall2018/test_images.npy\"\n",
        "    filename = \"test_images.npy\"\n",
        "\n",
        "    def __init__(self, root,\n",
        "                 transform=None, download=False):\n",
        "        self.root = os.path.expanduser(root)\n",
        "        self.transform = transform\n",
        "\n",
        "        if download:\n",
        "            self.download()\n",
        "\n",
        "        # now load the picked numpy arrays\n",
        "        file = os.path.join(self.root, self.filename)\n",
        "        self.test_data = np.load(file)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        img = self.test_data[index]\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        img = Image.fromarray(img.astype('uint8'))\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.test_data)\n",
        "\n",
        "    def download(self):\n",
        "        root = self.root\n",
        "        if not os.path.exists(os.path.join(root, self.filename)):\n",
        "            print(\"Downloading data...\")\n",
        "            urllib.request.urlretrieve(self.url, os.path.join(root, self.filename))\n",
        "            print(\"Download complete\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mYqoZsoN07xO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_preds_csv(model=\"mymodel_after5_2018-10-02 21:37:1917.pt\", test_dir=\"data\", test_batch_size=256, no_cuda=False):\n",
        "\n",
        "  # Load CIFAR10 using torch data paradigm\n",
        "  kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "\n",
        "  cifar10_mean_color = [0.49131522, 0.48209435, 0.44646862]\n",
        "  # std dev of color across training images\n",
        "  cifar10_std_color = [0.01897398, 0.03039277, 0.03872553]\n",
        "\n",
        "  transform = transforms.Compose([\n",
        "                   transforms.ToTensor(),\n",
        "                   transforms.Normalize(cifar10_mean_color, cifar10_std_color),\n",
        "              ])\n",
        "  test_dataset = ChallengeData(test_dir, download=True, transform=transform)\n",
        "  # Datasets\n",
        "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, **kwargs)\n",
        "\n",
        "  if os.path.exists(model):\n",
        "      model = torch.load(model)\n",
        "  else:\n",
        "      print('Model path specified does not exst')\n",
        "      sys.exit(1)\n",
        "\n",
        "  # cross-entropy loss function\n",
        "  criterion = F.cross_entropy\n",
        "  if not no_cuda:\n",
        "      model.cuda()\n",
        "\n",
        "\n",
        "  def evaluate():\n",
        "      '''\n",
        "      Compute loss on test data.\n",
        "      '''\n",
        "      model.eval()\n",
        "      loader = test_loader\n",
        "      predictions = [] \n",
        "      for batch_i, batch in enumerate(loader):\n",
        "          data = batch\n",
        "          if not no_cuda:\n",
        "              data= data.cuda()\n",
        "          data = Variable(data, volatile=True)\n",
        "          output = model(data)\n",
        "          pred = output.data.max(1, keepdim=True)[1]\n",
        "          predictions += pred.reshape(-1).tolist()\n",
        "          print('Batch:{}'.format(batch_i))\n",
        "      return predictions\n",
        "\n",
        "  predictions = evaluate()\n",
        "\n",
        "  with open('predictions.csv', 'w') as csv_file:\n",
        "      csv_writer = csv.writer(csv_file, delimiter=',')\n",
        "      csv_writer.writerow(['image_id', 'label'])\n",
        "      for i, p in enumerate(predictions):\n",
        "          csv_writer.writerow([i, int(p)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ro3OjFag4jRj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# test_preds_csv()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}